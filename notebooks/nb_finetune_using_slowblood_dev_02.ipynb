{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kyledinh/gsb-mlops/blob/main/labs/codellama-13a/nb_fine_codellama_13b_gsb_repea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDdy-k2BVDAo"
   },
   "source": [
    "# Colab Finetuning Using Slowblood Package (DEV)\n",
    "\n",
    "- Install Packages\n",
    "- Settings and Vars\n",
    "- Load Quantized Models\n",
    "- Tokenizer\n",
    "- Peft\n",
    "- Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyio==4.1.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.1\n",
      "async-lru==2.0.4\n",
      "attrs==23.1.0\n",
      "Babel==2.13.1\n",
      "beautifulsoup4==4.12.2\n",
      "bleach==6.1.0\n",
      "blinker==1.4\n",
      "certifi==2023.11.17\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "comm==0.2.0\n",
      "cryptography==3.4.8\n",
      "dbus-python==1.2.18\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "distro==1.7.0\n",
      "entrypoints==0.4\n",
      "exceptiongroup==1.2.0\n",
      "executing==2.0.1\n",
      "fastjsonschema==2.19.0\n",
      "filelock==3.13.1\n",
      "fqdn==1.5.1\n",
      "fsspec==2023.10.0\n",
      "httplib2==0.20.2\n",
      "idna==3.6\n",
      "importlib-metadata==4.6.4\n",
      "ipykernel==6.26.0\n",
      "ipython==8.18.1\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==8.1.1\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "jeepney==0.7.1\n",
      "Jinja2==3.1.2\n",
      "json5==0.9.14\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.20.0\n",
      "jsonschema-specifications==2023.11.1\n",
      "jupyter-archive==3.4.0\n",
      "jupyter-contrib-core==0.4.2\n",
      "jupyter-contrib-nbextensions==0.7.0\n",
      "jupyter-events==0.9.0\n",
      "jupyter-highlight-selected-word==0.2.0\n",
      "jupyter-lsp==2.2.1\n",
      "jupyter-nbextensions-configurator==0.6.3\n",
      "jupyter_client==7.4.9\n",
      "jupyter_core==5.5.0\n",
      "jupyter_server==2.10.1\n",
      "jupyter_server_terminals==0.4.4\n",
      "jupyterlab==4.0.9\n",
      "jupyterlab-widgets==3.0.9\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.25.2\n",
      "keyring==23.5.0\n",
      "launchpadlib==1.10.16\n",
      "lazr.restfulclient==0.14.4\n",
      "lazr.uri==1.0.6\n",
      "lxml==4.9.3\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib-inline==0.1.6\n",
      "mistune==3.0.2\n",
      "more-itertools==8.10.0\n",
      "mpmath==1.3.0\n",
      "nbclassic==1.0.0\n",
      "nbclient==0.9.0\n",
      "nbconvert==7.11.0\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.5.8\n",
      "networkx==3.2.1\n",
      "notebook==6.5.5\n",
      "notebook_shim==0.2.3\n",
      "numpy==1.26.2\n",
      "nvidia-cublas-cu12==12.1.3.1\n",
      "nvidia-cuda-cupti-cu12==12.1.105\n",
      "nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "nvidia-cuda-runtime-cu12==12.1.105\n",
      "nvidia-cudnn-cu12==8.9.2.26\n",
      "nvidia-cufft-cu12==11.0.2.54\n",
      "nvidia-curand-cu12==10.3.2.106\n",
      "nvidia-cusolver-cu12==11.4.5.107\n",
      "nvidia-cusparse-cu12==12.1.0.106\n",
      "nvidia-nccl-cu12==2.18.1\n",
      "nvidia-nvjitlink-cu12==12.3.101\n",
      "nvidia-nvtx-cu12==12.1.105\n",
      "oauthlib==3.2.0\n",
      "overrides==7.4.0\n",
      "packaging==23.2\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "pexpect==4.9.0\n",
      "Pillow==10.1.0\n",
      "platformdirs==4.0.0\n",
      "prometheus-client==0.19.0\n",
      "prompt-toolkit==3.0.41\n",
      "psutil==5.9.6\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "Pygments==2.17.2\n",
      "PyGObject==3.42.1\n",
      "PyJWT==2.3.0\n",
      "pyparsing==2.4.7\n",
      "python-apt==2.4.0+ubuntu2\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "PyYAML==6.0.1\n",
      "pyzmq==24.0.1\n",
      "referencing==0.31.0\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rpds-py==0.13.1\n",
      "SecretStorage==3.3.1\n",
      "Send2Trash==1.8.2\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.5\n",
      "stack-data==0.6.3\n",
      "sympy==1.12\n",
      "terminado==0.18.0\n",
      "tinycss2==1.2.1\n",
      "tomli==2.0.1\n",
      "torch==2.1.1\n",
      "torchaudio==2.1.1\n",
      "torchvision==0.16.1\n",
      "tornado==6.3.3\n",
      "traitlets==5.13.0\n",
      "triton==2.1.0\n",
      "types-python-dateutil==2.8.19.14\n",
      "typing_extensions==4.8.0\n",
      "uri-template==1.3.0\n",
      "urllib3==2.1.0\n",
      "wadllib==1.3.6\n",
      "wcwidth==0.2.12\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.6.4\n",
      "widgetsnbextension==4.0.9\n",
      "zipp==1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Loaded local VERSION:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9798caf6ecf4aa9bed534b316a8d999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -qU git+https://github.com/huggingface/accelerate.git\n",
    "%pip install -qU git+https://github.com/huggingface/peft.git\n",
    "%pip install -qU git+https://github.com/huggingface/transformers.git\n",
    "# !pip install transformers==4.31 #temporary fix required owing to breaking changes on Aug 9th 2023\n",
    "%pip install -qU bitsandbytes\n",
    "%pip install -qU datasets\n",
    "%pip install -qU einops\n",
    "%pip install -qU flash-attn --no-build-isolation\n",
    "%pip install -qU huggingface_hub\n",
    "%pip install -qU langchain\n",
    "%pip install -qU pytesseract\n",
    "%pip install -qU pypdfium2\n",
    "%pip install -qU safetensors\n",
    "%pip install -qU torch\n",
    "%pip install -qU xformers\n",
    "\n",
    "# %pip install -q -U slowblood\n",
    "## IMPORTING THE UPLOADED module from pypi/src/slowblood to /workspace/slowblood\n",
    "import os\n",
    "os.chdir(os.path.expanduser(\"/workspace\"))\n",
    "import slowblood\n",
    "print(\"Loaded local VERSION: \", slowblood.lib_settings.VERSION)\n",
    "\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# LOGIN TO HUGGINGFACE WITH ACCES TOKEN\n",
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and Vars\n",
    "\n",
    "- SETTINGS, configuration for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version:  2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(slowblood)\n",
    "print(\"version: \", slowblood.lib_settings.VERSION)\n",
    "\n",
    "%pip freeze | grep slowblood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = slowblood.FineTuningSettings(\n",
    "  base_model=\"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "  new_model_name=\"gsb-repea-codellama-13b-ft\",\n",
    "  repo_name=\"Slowblood\",\n",
    "  dataset=\"Slowblood/gsb-rapid-entry-pea-v2\",\n",
    "  cache_dir=\"/workspace/cache\"\n",
    ")\n",
    "\n",
    "print(SETTINGS.new_model)\n",
    "print(SETTINGS.adapter_model)\n",
    "print(\"VERSION: \", slowblood.lib_settings.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wgbsa4U2TBk"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alLhGWu9Ngvg"
   },
   "source": [
    "# Load Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz4eBuxZHi7F"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Now you can create the model with the modified configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SETTINGS.base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0},\n",
    "    # device_map=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=SETTINGS.cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiScaA1acm9J"
   },
   "outputs": [],
   "source": [
    "print(transformers.__version__)\n",
    "!pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emcg4xfjxvaZ"
   },
   "source": [
    "## Set Up Tokenizer\n",
    "\n",
    "Get tokenizer, then set pad token, update the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEitzH351wCR"
   },
   "outputs": [],
   "source": [
    "tokenizer = slowblood.getTokenizerForModel(SETTINGS.base_model, cache_dir=SETTINGS.cache_dir, debug=True)\n",
    "model = slowblood.updateModelWithTokenizer(model, tokenizer, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSuBrec41b0-"
   },
   "source": [
    "## Set up LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWYusaRvperY"
   },
   "outputs": [],
   "source": [
    "# #only if you want to load with adapters (it can be faster to download a base model and add adapters)\n",
    "# from peft import PeftModel\n",
    "\n",
    "# # adapter_model = f'{base_model}' + '-function-calling-adapters-v2'\n",
    "# adapter_model = \"RonanMcGovern/Llama-2-13b-chat-hf-function-calling-adapters-v2\"\n",
    "\n",
    "# # load peft model with adapters\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     adapter_model,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hd22_v_mkbZy"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOhd6empkd0_"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EBGtk1On8tc"
   },
   "outputs": [],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UkOrtBBohnL"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "      \"self_attn.q_proj\",\n",
    "      \"self_attn.k_proj\",\n",
    "      \"self_attn.v_proj\",\n",
    "      \"self_attn.o_proj\",\n",
    "      \"self_attn.rotary_emb.inv_freq\",\n",
    "      \"mlp.gate_proj\",\n",
    "      \"mlp.up_proj\",\n",
    "      \"mlp.down_proj\",\n",
    "      \"input_layernorm.weight\",\n",
    "      \"post_attention_layernorm.weight\",\n",
    "      \"model.norm.weight\",\n",
    "      \"lm_head.weight\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxDuoyQvQfD2"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cc3LsTLFjAGT"
   },
   "outputs": [],
   "source": [
    "%pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZbrRk4J3qVH"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    SETTINGS.dataset # \"Slowblood/gsb-rapid-entry-pea\",\n",
    "    # revision=\"functionList\" # optionally specify a branch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIVJPMTBAzID"
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNyFtLkj1G5f"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, response_lengths, input_lengths):\n",
    "        self.encodings = encodings\n",
    "        self.response_lengths = response_lengths\n",
    "        self.input_lengths = input_lengths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "\n",
    "        # Set labels to be the same as input_ids\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "\n",
    "        # Calculate the start and end positions of the response\n",
    "        response_start_position = self.input_lengths[idx]\n",
    "        response_end_position = self.input_lengths[idx] + self.response_lengths[idx]\n",
    "\n",
    "        # Create a loss mask that covers only the response tokens\n",
    "        item[\"loss_mask\"] = torch.zeros_like(item[\"input_ids\"])\n",
    "        item[\"loss_mask\"][response_start_position:response_end_position] = 1\n",
    "\n",
    "        # Shift the loss mask to the left by one position\n",
    "        shifted_loss_mask = torch.cat([item[\"loss_mask\"][1:], torch.tensor([0])])\n",
    "        item[\"loss_mask\"] = shifted_loss_mask\n",
    "\n",
    "        # Shift the labels to the left by one position\n",
    "        item[\"labels\"][:-1] = item[\"input_ids\"][1:]\n",
    "\n",
    "        # Replace the token after the response with an EOS token\n",
    "        item[\"labels\"][response_end_position - 1] = 2\n",
    "\n",
    "        # Replace the token after the response with an 1 in the loss mask\n",
    "        item[\"loss_mask\"][response_end_position - 1] = 1\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewsa4W2Z1HfB"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer):\n",
    "    # Define the roles and markers\n",
    "    B_FUNC, E_FUNC = \"<FUNCTIONS>\", \"</FUNCTIONS>\"\n",
    "    B_SYS, E_SYS = \"<SYS>\", \"</SYS>\"\n",
    "    B_USER, E_USER = \"<USER>\", \"</USER>\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "\n",
    "    # Create the formatted text with the correct roles for each part of the dialogue\n",
    "    formatted_dataset = dataset.map(\n",
    "        lambda x: {\n",
    "            \"input_text\": \"\".join([\n",
    "                f\"{B_SYS}{x['systemPrompt'].strip()}{E_SYS}\",\n",
    "                f\"{B_INST} {x['userPrompt'].strip()} {E_INST}\\n\\n\",\n",
    "                f\"{x['assistantResponse'].strip()}\",  # appending the EOS token in TextData...\n",
    "            ]),\n",
    "            \"response_text\": \"\".join([\n",
    "                f\"{x['assistantResponse'].strip()}\",  # appending the EOS token in TextData...\n",
    "            ]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    encodings = tokenizer([dialogue[\"input_text\"] for dialogue in formatted_dataset], truncation=True, padding=True, max_length=1024, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "    # Tokenize the response one by one without padding and special tokens for the purpose of calculating length\n",
    "    response_lengths = [len(tokenizer.encode(dialogue[\"response_text\"], truncation=True, max_length=1024, padding=False, add_special_tokens=False)) for dialogue in formatted_dataset]\n",
    "\n",
    "    # Tokenize the input one by one without padding and with the initial special token for the purpose of calculating length\n",
    "    total_lengths = [len(tokenizer.encode(dialogue[\"input_text\"], truncation=True, max_length=1024, padding=False, add_special_tokens=True)) for dialogue in formatted_dataset]\n",
    "    input_lengths = [total_length - response_length for total_length, response_length in zip(total_lengths, response_lengths)]\n",
    "\n",
    "    # Create TextDataset\n",
    "    text_dataset = TextDataset(encodings, response_lengths, input_lengths)\n",
    "\n",
    "    return text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRB550G01J5b"
   },
   "outputs": [],
   "source": [
    "# Apply function to your datasets\n",
    "train_dataset = prepare_dataset(data['train'], tokenizer)\n",
    "test_dataset = prepare_dataset(data['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLmXcTTV2RdD"
   },
   "source": [
    "### Examine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaR9H_pH2fOg"
   },
   "outputs": [],
   "source": [
    "# Print the number of items in the dataset\n",
    "print(f\"Number of samples in the dataset: {len(train_dataset)}\")\n",
    "\n",
    "# Get a sample item\n",
    "sample_item = train_dataset[11]  # replace 0 with the index of the sample you want to examine\n",
    "\n",
    "# Print the dimensions of the sample item\n",
    "print(f\"Dimensions of input_ids: {sample_item['input_ids'].shape}\")\n",
    "print(f\"Dimensions of attention_mask: {sample_item['attention_mask'].shape}\")\n",
    "print(f\"Dimensions of loss_mask: {sample_item['loss_mask'].shape}\")\n",
    "print(f\"Dimensions of labels: {sample_item['labels'].shape}\")\n",
    "\n",
    "# Print some tokens from the start and end of the sample\n",
    "num_tokens_to_print = 336  # replace with the number of tokens you want to print\n",
    "\n",
    "print(\"\\nTokens at the start of the sample:\")\n",
    "print(sample_item['input_ids'][:num_tokens_to_print].tolist())\n",
    "print(tokenizer.convert_ids_to_tokens(sample_item['input_ids'][:num_tokens_to_print].tolist()))\n",
    "\n",
    "print(\"\\nLabels at the start of the sample:\")\n",
    "print(sample_item['labels'][:num_tokens_to_print].tolist())\n",
    "print(tokenizer.convert_ids_to_tokens(sample_item['labels'][:num_tokens_to_print].tolist()))\n",
    "\n",
    "print(\"Attention mask at the start of the sample:\")\n",
    "print(sample_item['attention_mask'][:num_tokens_to_print].tolist())\n",
    "\n",
    "print(\"Loss mask at the start of the sample:\")\n",
    "print(sample_item['loss_mask'][:num_tokens_to_print].tolist())\n",
    "\n",
    "print(\"\\nTokens at the end of the sample:\")\n",
    "print(sample_item['input_ids'][-num_tokens_to_print:].tolist())\n",
    "print(tokenizer.convert_ids_to_tokens(sample_item['input_ids'][-num_tokens_to_print:].tolist()))\n",
    "\n",
    "print(\"\\nLabels at the end of the sample:\")\n",
    "print(sample_item['labels'][-num_tokens_to_print:].tolist())\n",
    "print(tokenizer.convert_ids_to_tokens(sample_item['labels'][-num_tokens_to_print:].tolist()))\n",
    "\n",
    "print(\"Attention mask at the end of the sample:\")\n",
    "print(sample_item['attention_mask'][-num_tokens_to_print:].tolist())\n",
    "\n",
    "print(\"Loss mask at the end of the sample:\")\n",
    "print(sample_item['loss_mask'][-num_tokens_to_print:].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6giTvOr2atI"
   },
   "source": [
    "## Generate a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlZGi8K1Hio8"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hac875Cgz8Zz"
   },
   "outputs": [],
   "source": [
    "import re  # import regular expressions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOhG_TJ9EWwz"
   },
   "outputs": [],
   "source": [
    "def generate(index):\n",
    "    # Define the roles and markers\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<SYS>\", \"</SYS>\\n\\n\"\n",
    "    B_FUNC, E_FUNC = \"<FUNCTIONS>\", \"</FUNCTIONS>\\n\\n\"\n",
    "\n",
    "    # functionList = data['test'][index]['functionList']\n",
    "    # user_prompt = data['test'][index]['userPrompt']\n",
    "    # correct_answer = data['test'][index]['assistantResponse']\n",
    "\n",
    "    # Format your prompt template\n",
    "    system_prompt = data['train'][index]['systemPrompt']\n",
    "    user_prompt = data['train'][index]['userPrompt']\n",
    "    correct_answer = data['train'][index]['assistantResponse']\n",
    "\n",
    "    # Format your prompt template\n",
    "    prompt = f\"{B_SYS}{system_prompt.strip()}{E_SYS}{B_INST} {user_prompt.strip()} {E_INST}\\n\\n\"\n",
    "\n",
    "    print(\"Prompt:\")\n",
    "    print(prompt)\n",
    "\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    output = model.generate(input_ids=encoding.input_ids,\n",
    "                            attention_mask=encoding.attention_mask,\n",
    "                            max_new_tokens=2000,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.01,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            top_k=0)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Subtract the length of input_ids from output to get only the model's response\n",
    "    output_text = tokenizer.decode(output[0, len(encoding.input_ids[0]):], skip_special_tokens=False)\n",
    "    output_text = re.sub('\\n+', '\\n', output_text)  # remove excessive newline characters\n",
    "\n",
    "    print(\"Generated Assistant Response:\")\n",
    "    print(output_text)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Correct Assistant Response:\")\n",
    "    print(correct_answer)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pRFK3thQ1XV"
   },
   "outputs": [],
   "source": [
    "generate(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J53zPp1qSdZp"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSjTZHnSUFEU"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMiT3lsLz-zc"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(transformers.Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Define the number of tokens you want to display\n",
    "        num_tokens = 25  # This displays info on the actual and predicted tokens at the end of each sequence.\n",
    "\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        loss_mask = inputs.pop(\"loss_mask\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Check for NaN in logits and labels\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"NaN detected in logits\")\n",
    "            print(logits)\n",
    "\n",
    "        # Convert logits to probabilities using softmax function\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the most probable tokens\n",
    "        predicted_token_ids = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "        losses = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        # Reshaping the losses to have dimensions [batch_size, seq_length]\n",
    "        losses = losses.view(-1, inputs['input_ids'].size(1))\n",
    "\n",
    "        # Apply the loss mask\n",
    "        masked_loss = losses * loss_mask\n",
    "\n",
    "        # Check for NaN in losses and zero in loss_mask.sum()\n",
    "        if torch.isnan(losses).any():\n",
    "            print(\"NaN detected in losses\")\n",
    "            # print(losses)\n",
    "\n",
    "        if loss_mask.sum() == 0:\n",
    "            print(\"Sum of loss_mask is zero\")\n",
    "            return (torch.tensor(0).to(loss_mask.device), outputs) if return_outputs else torch.tensor(0).to(loss_mask.device)  # Early return\n",
    "\n",
    "        # Aggregate the masked losses\n",
    "        loss = masked_loss.sum() / (loss_mask.sum() + 1e-9)  # normalizing by the number of tokens considered + epsilon to prevent division by zero\n",
    "\n",
    "        # Print formatted tokens\n",
    "        batch_size, seq_length = inputs['input_ids'].size()\n",
    "\n",
    "        # num_tokens = len(inputs['input_ids'][0])\n",
    "\n",
    "        # # Useful for debugging training - recommend just training a small number of steps\n",
    "        # print(\"-\" * 120)\n",
    "        # print(f\"Token analysis for last {num_tokens} tokens:\")\n",
    "        # header_format = \"{:<10}{:<20}{:<20}{:<20}{:<20}{:<30}{:<30}\".format(\"Index\", \"Input Token\", \"Predicted Token\", \"True Token\", \"Loss Mask\", \"Raw Loss\", \"Masked Loss\")\n",
    "\n",
    "        # for batch_idx in range(batch_size):\n",
    "        #     input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][batch_idx])  # Using batch_idx\n",
    "        #     predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[batch_idx])  # Using batch_idx\n",
    "        #     true_tokens = tokenizer.convert_ids_to_tokens(labels[batch_idx])  # Using batch_idx\n",
    "\n",
    "        #     print(f\"\\nBatch {batch_idx + 1} of {batch_size}:\")\n",
    "        #     print(header_format)\n",
    "        #     for i in range(-num_tokens, 0, 1):\n",
    "        #         index = seq_length + i  # Correct index based on sequence length\n",
    "        #         print(\"{:<10}{:<20}{:<20}{:<20}{:<20.1f}{:<30.6f}{:<30.6f}\".format(index, input_tokens[index], predicted_tokens[index], true_tokens[index], loss_mask[batch_idx, i].item(), losses[batch_idx, i], masked_loss[batch_idx, i]))\n",
    "        #     print(\"-\" * 120)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "      train_dataset = self.train_dataset\n",
    "      data_collator = self.data_collator\n",
    "\n",
    "      dataloader_params = {\n",
    "          \"batch_size\": self.args.train_batch_size,\n",
    "          \"collate_fn\": data_collator,\n",
    "          \"num_workers\": self.args.dataloader_num_workers,\n",
    "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "      }\n",
    "\n",
    "      if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "          dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "          dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "\n",
    "      return DataLoader(train_dataset, **dataloader_params)\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "      eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "      if eval_dataset is None:\n",
    "          raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "\n",
    "      data_collator = self.data_collator\n",
    "\n",
    "      # Parameters for the DataLoader\n",
    "      dataloader_params = {\n",
    "          \"batch_size\": self.args.eval_batch_size,\n",
    "          \"collate_fn\": data_collator,\n",
    "          \"num_workers\": self.args.dataloader_num_workers,\n",
    "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "      }\n",
    "\n",
    "      # If your dataset isn't an instance of torch's IterableDataset, you can provide sampler and drop_last\n",
    "      if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "          dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
    "          dataloader_params[\"drop_last\"] = False  # Typically, you don't drop the last batch for evaluation\n",
    "\n",
    "      return DataLoader(eval_dataset, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz9auc1jCvBg"
   },
   "outputs": [],
   "source": [
    "class CustomDataCollator: # Needed if the EOS token is to be included in training.\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        loss_mask = torch.stack([item['loss_mask'] for item in batch])\n",
    "\n",
    "        # # Debugging: print details of the first sequence in the batch\n",
    "        # num_elements_to_view = 20  # Number of last elements to view\n",
    "\n",
    "        # # Decoding the input_ids\n",
    "        # decoded_input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "        # print(\"Debugging last\", num_elements_to_view, \"elements of the first sequence in the batch:\")\n",
    "        # print(\"{:<20}{:<20}{:<20}{:<20}\".format(\"Token\", \"Input ID\", \"Label\", \"Loss Mask\"))\n",
    "        # for i in range(-num_elements_to_view, 0, 1):\n",
    "        #   print(\"{:<20}{:<20}{:<20}{:<20}\".format(decoded_input_tokens[i], input_ids[0, i].item(), labels[0, i].item(), loss_mask[0, i].item()))\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'loss_mask': loss_mask\n",
    "        }\n",
    "\n",
    "data_collator = CustomDataCollator(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDniJ-bq4nxD"
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset, #turn on the eval dataset if you want to use evaluation functionality versus the test dataset (not provided in this script)\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        max_grad_norm=1,\n",
    "        warmup_ratio=0.1,\n",
    "        eval_steps=0.2,\n",
    "        # max_steps=3,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type='cosine',\n",
    "    ),\n",
    "    data_collator=data_collator,\n",
    "    # data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsjbF5SZ6ifV"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWXT-FNfSlpk"
   },
   "source": [
    "# Example After Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9vrd1FXSz8W"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MezO1msTSnEs"
   },
   "outputs": [],
   "source": [
    "generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJvV8blW_qzr"
   },
   "outputs": [],
   "source": [
    "generate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6FqNGBAz7ct"
   },
   "source": [
    "# Merge Adapters and Save Model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e19FlnnCBlrM"
   },
   "outputs": [],
   "source": [
    "def getTokenFromFile(file):\n",
    "    with open(file, 'r') as file:\n",
    "        data = file.read().rstrip()\n",
    "    return data\n",
    "\n",
    "HF_TOKEN = getTokenFromFile(\"/root/.cache/huggingface/token\")\n",
    "\n",
    "print(HF_TOKEN[0:9])\n",
    "# Save the model\n",
    "model.save_pretrained(SETTINGS.adapter_model, push_to_hub=True)\n",
    "\n",
    "# Push the model to the hub\n",
    "model.push_to_hub(SETTINGS.adapter_model, private=True, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZPgnENAHmuN"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, PretrainedConfig\n",
    "# import torch\n",
    "\n",
    "# reload the base model (you might need a pro subscription for this because you may need a high RAM environment since this is loading the full original model, not quantized)\n",
    "model = AutoModelForCausalLM.from_pretrained(SETTINGS.base_model, device_map='cpu', trust_remote_code=True, torch_dtype=torch.float16, cache_dir=SETTINGS.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrwiiYLqG6sp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# load perf model with new adapters\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    SETTINGS.adapter_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOQybEtGHJ9v"
   },
   "outputs": [],
   "source": [
    "model = model.merge_and_unload() # merge adapters with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTKqNQQpIxL6"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(SETTINGS.new_model, use_auth_token=True, max_shard_size=\"10GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buqIU-9VJxVV"
   },
   "outputs": [],
   "source": [
    "#Push the tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(SETTINGS.base_model, trust_remote_code=True)\n",
    "tokenizer.push_to_hub(SETTINGS.new_model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHdmq8Lrshsz"
   },
   "source": [
    "# Known Issues/Improvements\n",
    "- No open issues."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
